{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ConvRNNCell(object):\n",
    "  \"\"\"Abstract object representing an Convolutional RNN cell.\"\"\"\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Run this RNN cell on inputs, starting from the given state.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    \"\"\"size(s) of state(s) used by this cell.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    \"\"\"Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n",
    "    raise NotImplementedError(\"Abstract method\")\n",
    "\n",
    "  def zero_state(self, batch_size, dtype):\n",
    "    \"\"\"Return zero-filled state tensor(s).\n",
    "    Args:\n",
    "      batch_size: int, float, or unit Tensor representing the batch size.\n",
    "      dtype: the data type to use for the state.\n",
    "    Returns:\n",
    "      tensor of shape '[batch_size x shape[0] x shape[1] x num_features]\n",
    "      filled with zeros\n",
    "    \"\"\"\n",
    "    shape = self.shape \n",
    "    num_features = self.num_features\n",
    "    zeros = tf.zeros([batch_size, shape[0], shape[1], num_features * 2]) \n",
    "    return zeros\n",
    "\n",
    "class BasicConvLSTMCell(ConvRNNCell):\n",
    "  \"\"\"Basic Conv LSTM recurrent network cell. The\n",
    "  \"\"\"\n",
    "  def __init__(self, shape, filter_size, num_features, forget_bias=1.0, input_size=None,\n",
    "               state_is_tuple=False, activation=tf.nn.tanh):\n",
    "    \"\"\"Initialize the basic Conv LSTM cell.\n",
    "    Args:\n",
    "      shape: int tuple thats the height and width of the cell\n",
    "      filter_size: int tuple thats the height and width of the filter\n",
    "      num_features: int thats the depth of the cell \n",
    "      forget_bias: float, The bias added to forget gates (see above).\n",
    "      input_size: Deprecated and unused.\n",
    "      state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "        the `c_state` and `m_state`.  If False, they are concatenated\n",
    "        along the column axis.  The latter behavior will soon be deprecated.\n",
    "      activation: Activation function of the inner states.\n",
    "    \"\"\"\n",
    "    #if not state_is_tuple:\n",
    "      #logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "      #             \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "    if input_size is not None:\n",
    "      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "    self.shape = shape \n",
    "    self.filter_size = filter_size\n",
    "    self.num_features = num_features \n",
    "    self._forget_bias = forget_bias\n",
    "    self._state_is_tuple = state_is_tuple\n",
    "    self._activation = activation\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (LSTMStateTuple(self._num_units, self._num_units)\n",
    "            if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "    with tf.variable_scope(scope or type(self).__name__):  # \"BasicLSTMCell\"\n",
    "      # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "      if self._state_is_tuple:\n",
    "        c, h = state\n",
    "      else:\n",
    "        c, h = tf.split(axis=3, num_or_size_splits=2, value=state)\n",
    "      concat = _conv_linear([inputs, h], self.filter_size, self.num_features * 4, True)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=concat)\n",
    "\n",
    "      new_c = (c * tf.nn.sigmoid(f + self._forget_bias) + tf.nn.sigmoid(i) *\n",
    "               self._activation(j))\n",
    "      new_h = self._activation(new_c) * tf.nn.sigmoid(o)\n",
    "\n",
    "      if self._state_is_tuple:\n",
    "        new_state = LSTMStateTuple(new_c, new_h)\n",
    "      else:\n",
    "        new_state = tf.concat(axis=3, values=[new_c, new_h])\n",
    "      return new_h, new_state\n",
    "\n",
    "def _conv_linear(args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n",
    "  \"\"\"convolution:\n",
    "  Args:\n",
    "    args: a 4D Tensor or a list of 4D, batch x n, Tensors.\n",
    "    filter_size: int tuple of filter height and width.\n",
    "    num_features: int, number of features.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 4D Tensor with shape [batch h w num_features]\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the total size of arguments on dimension 1.\n",
    "  total_arg_size_depth = 0\n",
    "  shapes = [a.get_shape().as_list() for a in args]\n",
    "  for shape in shapes:\n",
    "    if len(shape) != 4:\n",
    "      raise ValueError(\"Linear is expecting 4D arguments: %s\" % str(shapes))\n",
    "    if not shape[3]:\n",
    "      raise ValueError(\"Linear expects shape[4] of arguments: %s\" % str(shapes))\n",
    "    else:\n",
    "      total_arg_size_depth += shape[3]\n",
    "\n",
    "  dtype = [a.dtype for a in args][0]\n",
    "\n",
    "  # Now the computation.\n",
    "  with tf.variable_scope(scope or \"Conv\"):\n",
    "    matrix = tf.get_variable(\n",
    "        \"Matrix\", [filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype)\n",
    "    if len(args) == 1:\n",
    "      res = tf.nn.conv2d(args[0], matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    else:\n",
    "      res = tf.nn.conv2d(tf.concat(axis=3, values=args), matrix, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if not bias:\n",
    "      return res\n",
    "    bias_term = tf.get_variable(\n",
    "        \"Bias\", [num_features],\n",
    "        dtype=dtype,\n",
    "        initializer=tf.constant_initializer(\n",
    "            bias_start, dtype=dtype))\n",
    "  return res + bias_term\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
